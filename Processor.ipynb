{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitsekarpramesivirtualenvb95a2fb76c754aa08b6ecdb9ac4dccd6",
   "display_name": "Python 3.7.3 64-bit ('Sekarpramesi': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unittest, time, re,os,threading\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import glob\n",
    "from settings import TWITTER_USERNAME,TWITTER_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagProMahasiswa=[\"#kpkkuatkorupsiturun\", \"#KekuasaanDitanganRakyat\", \"#GejayanMemanggil2\", \"#diperkosanegara\", \"#mahasiswabergerak\", \"#hidupmahasiswa\", \"#tolakruukuhp\", \"#tolakrevisiuukpk\", \"#hiduprakyatindonesia\", \"#mositidakpercaya\", \"#reformasidikorupsi\", \"#stmmelawan\", \"#stmbergerak\", \"#gejayanmemanggil\", \"#indonesiaberduka\", \"#bebaskananandabadudu\", \"#BebaskanDandhyLaksono\", \"#savekpk\", \"#petisisavekpk\",\"#belajardaripelajar\",\"#rakyatbergerak\",\"#revisiuukpkfornkri\", \"#dukungrevisiuukpk\", \"#kpkcengeng\", \"#tempokacungkpk\", \"#kpkpatuhaturan\", \"#kpklebihbaik\", \"#sayabersamajokowi\", \"#jokowimendengarrakyat\", \"#kitadukungjokowi\",\"#percayalangkahjokowi\"]\n",
    "hashtagProMahasiswa = [item.lower() for item in hashtagProMahasiswa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"*.csv\")\n",
    "li = []\n",
    "for filename in all_files:\n",
    "   df = pd.read_csv(filename, index_col=None, header=0)\n",
    "   li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "frame.to_csv('combined.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined.csv\")\n",
    "df.columns\n",
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1','time','extraInfoTargetUser', 'extraInfoContent','content', 'lang', 'mentions','contentUrl'],axis =1)\n",
    "print(\"Shape : \"+str(df.shape)) #overall shape\n",
    "#remove duplicate column\n",
    "dfDrop = df.copy(deep=True)\n",
    "dfDrop = dfDrop.drop_duplicates(subset=\"urlTweet\")\n",
    "print(\"Dropped Shape : \"+str(dfDrop.shape))\n",
    "#remove empty column\n",
    "dfDrop = dfDrop.dropna(subset=['hashtags'])\n",
    "print(\"Empty Row Dropped Shape : \"+str(dfDrop.shape))\n",
    "indexNames = dfDrop[ dfDrop['hashtags'] == \"[]\" ].index\n",
    "dfDrop.drop(indexNames,inplace =True)\n",
    "print(\"Empty Row [] Dropped Shape : \"+str(dfDrop.shape))\n",
    "dfDrop['hashtags']= dfDrop['hashtags'].str.lower()\n",
    "dfDrop.head(5)\n",
    "dfDrop.to_csv(\"processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# d={}\n",
    "# d[('Antje','place')]='Barcelona'\n",
    "# d[('Antje','year')]=1987\n",
    "# d[('Antje','YEAR')]=1988\n",
    "# d[('Mike','place')]='Berlin'\n",
    "# key=('Antje','year')\n",
    "# print(d)\n",
    "# if key in d:\n",
    "#     d[key]+=1\n",
    "# else:\n",
    "#     d[key]='a'\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accumulate interaction method\n",
    "myDict={}\n",
    "x = 0\n",
    "for index,row in dfDrop.iterrows():\n",
    "    ht = row['hashtags'].strip('[')\n",
    "    ht = ht.strip(']')\n",
    "    ht = ht.replace(\"'\",\"\")\n",
    "    ht = ht.replace(\" \",\"\")\n",
    "    htArr = ht.split(',')\n",
    "    for h in htArr:\n",
    "        if h not in hashtagProMahasiswa:\n",
    "            break;\n",
    "        else:\n",
    "            key = (row['username'],h)\n",
    "            if key in myDict:\n",
    "                myDict[key]+=1\n",
    "            else:\n",
    "                myDict[key]=1\n",
    "for t in myDict:\n",
    "    print(t[0])\n",
    "    print(t[1])\n",
    "    print(myDict[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagCount = len(hashtagProMahasiswa)\n",
    "usernameCount = dfDrop.username.unique().shape[0]\n",
    "usernames = dfDrop.username.unique()\n",
    "usernameDict = {}\n",
    "hashtagDict = {}\n",
    "file = open(\"csvToPajek.net\",\"w\")\n",
    "file.write(\"*vertices     %d %d\\n\"%(hashtagCount+len(usernames),len(usernames)))\n",
    "#write users\n",
    "for x in range(len(usernames)):\n",
    "    usernameDict[usernames[x]]=x+1\n",
    "    file.write(\"     %d \\\"%s\\\"\\n\"%(x+1,usernames[x]))\n",
    "#write hashtags\n",
    "for x in range(hashtagCount):\n",
    "    hashtagDict[hashtagProMahasiswa[x]]=x+len(usernames)+1\n",
    "    file.write(\"     %d \\\"%s\\\"\\n\"%(x+len(usernames)+1,hashtagProMahasiswa[x]))\n",
    "file.write(\"*Edges\\n\")\n",
    "#write Edges\n",
    "for x in myDict:\n",
    "    userNameId = usernameDict[x[0]]\n",
    "    hashTagId = hashtagDict[x[1]]\n",
    "    weight = myDict[x]\n",
    "    file.write(\"     %s  %s   %d\\n\"%(userNameId,hashTagId,weight))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_source=[]\n",
    "_target=[]\n",
    "_weight=[]\n",
    "_id=[]\n",
    "gephiDict = {\n",
    "    \"ID\":_id,\n",
    "    \"Source\":_source,\n",
    "    \"Target\":_target,\n",
    "    \"Weight\":_weight,\n",
    "}\n",
    "i = 1\n",
    "for x in myDict:\n",
    "    _id.append(i)\n",
    "    _source.append(x[0])\n",
    "    _target.append(x[1])\n",
    "    _weight.append(myDict[x])\n",
    "    i+=1\n",
    "gephi = pd.DataFrame.from_dict(gephiDict,orient='index').T \n",
    "gephi.to_csv(\"gephiNew.csv\",index=False)"
   ]
  }
 ]
}